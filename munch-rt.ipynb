{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End PyTorch -> ONNX -> CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.onnx\n",
    "\n",
    "import fast_neural_style.neural_style.utils as utils\n",
    "from fast_neural_style.neural_style.transformer_net import TransformerNet\n",
    "from fast_neural_style.neural_style.vgg import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg = models.vgg16(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training unavailable... using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, using GPU.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('GPU training unavailable... using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "image_size = 256\n",
    "style_size = 256\n",
    "\n",
    "epochs = 3\n",
    "dataset = 'fast_neural_style/images/train-images/'\n",
    "batch_size = 4\n",
    "lr = 1e-3\n",
    "# If starting from existing model\n",
    "# model_path = 'fast_neural_style/snapshots/epoch_1000_Fri_Jul_12_18:53:26_2019_100000.0_10000000000.0.model'\n",
    "model_path = 'models/udnie.pth'\n",
    "\n",
    "checkpoint_model_dir = './snapshots'\n",
    "checkpoint_interval = 20\n",
    "\n",
    "content_weight = 3\n",
    "style_weight = 1000\n",
    "\n",
    "style_image = 'fast_neural_style/images/style-images/scream_painting.jpg'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    dataset, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation network.\n",
    "transformer = TransformerNet()\n",
    "\n",
    "state_dict = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.InstanceNorm2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.conv2d.weight', 'conv1.conv2d.bias', 'in1.weight', 'in1.bias', 'in1.running_mean', 'in1.running_var', 'conv2.conv2d.weight', 'conv2.conv2d.bias', 'in2.weight', 'in2.bias', 'in2.running_mean', 'in2.running_var', 'conv3.conv2d.weight', 'conv3.conv2d.bias', 'in3.weight', 'in3.bias', 'in3.running_mean', 'in3.running_var', 'res1.conv1.conv2d.weight', 'res1.conv1.conv2d.bias', 'res1.in1.weight', 'res1.in1.bias', 'res1.in1.running_mean', 'res1.in1.running_var', 'res1.conv2.conv2d.weight', 'res1.conv2.conv2d.bias', 'res1.in2.weight', 'res1.in2.bias', 'res1.in2.running_mean', 'res1.in2.running_var', 'res2.conv1.conv2d.weight', 'res2.conv1.conv2d.bias', 'res2.in1.weight', 'res2.in1.bias', 'res2.in1.running_mean', 'res2.in1.running_var', 'res2.conv2.conv2d.weight', 'res2.conv2.conv2d.bias', 'res2.in2.weight', 'res2.in2.bias', 'res2.in2.running_mean', 'res2.in2.running_var', 'res3.conv1.conv2d.weight', 'res3.conv1.conv2d.bias', 'res3.in1.weight', 'res3.in1.bias', 'res3.in1.running_mean', 'res3.in1.running_var', 'res3.conv2.conv2d.weight', 'res3.conv2.conv2d.bias', 'res3.in2.weight', 'res3.in2.bias', 'res3.in2.running_mean', 'res3.in2.running_var', 'res4.conv1.conv2d.weight', 'res4.conv1.conv2d.bias', 'res4.in1.weight', 'res4.in1.bias', 'res4.in1.running_mean', 'res4.in1.running_var', 'res4.conv2.conv2d.weight', 'res4.conv2.conv2d.bias', 'res4.in2.weight', 'res4.in2.bias', 'res4.in2.running_mean', 'res4.in2.running_var', 'res5.conv1.conv2d.weight', 'res5.conv1.conv2d.bias', 'res5.in1.weight', 'res5.in1.bias', 'res5.in1.running_mean', 'res5.in1.running_var', 'res5.conv2.conv2d.weight', 'res5.conv2.conv2d.bias', 'res5.in2.weight', 'res5.in2.bias', 'res5.in2.running_mean', 'res5.in2.running_var', 'deconv1.conv2d.weight', 'deconv1.conv2d.bias', 'in4.weight', 'in4.bias', 'in4.running_mean', 'in4.running_var', 'deconv2.conv2d.weight', 'deconv2.conv2d.bias', 'in5.weight', 'in5.bias', 'in5.running_mean', 'in5.running_var', 'deconv3.conv2d.weight', 'deconv3.conv2d.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove deprecated InstanceNorm2D elements from state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "pruned_state_dict = copy.copy(state_dict)\n",
    "\n",
    "items_to_remove = []\n",
    "for k in pruned_state_dict.keys():\n",
    "    if k.split('.')[-1] in ['running_mean', 'running_var']:\n",
    "        items_to_remove.append(k)\n",
    "\n",
    "for item in items_to_remove:\n",
    "    pruned_state_dict.pop(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNet(\n",
       "  (conv1): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "    (conv2d): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1))\n",
       "  )\n",
       "  (in1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv2): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       "  (in2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv3): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       "  (in3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (res1): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res2): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res3): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res4): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res5): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (deconv1): UpsampleConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (in4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (deconv2): UpsampleConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (in5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (deconv3): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "    (conv2d): Conv2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(pruned_state_dict, strict=False)\n",
    "\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup optimizer and loss network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(transformer.parameters(), lr=lr)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Loss Network: VGG16\n",
    "vgg = Vgg16(requires_grad=False).to(device)\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "style = utils.load_image(style_image, size=style_size)\n",
    "style = style_transform(style)\n",
    "style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "features_style = vgg(utils.normalize_batch(style))\n",
    "gram_style = [utils.gram_matrix(y) for y in features_style]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    transformer.train()\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    count = 0\n",
    "    for batch_id, (x, _) in enumerate(train_loader):\n",
    "        n_batch = len(x)\n",
    "        count += n_batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # CUDA if available\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Transform image\n",
    "        y = transformer(x)\n",
    "\n",
    "        y = utils.normalize_batch(y)\n",
    "        x = utils.normalize_batch(x)\n",
    "\n",
    "        # Feature Map of original image\n",
    "        features_x = vgg(x)\n",
    "        # Feature Map of transformed image\n",
    "        features_y = vgg(y)\n",
    "\n",
    "        # Difference between transformed image, original image.\n",
    "        content_loss = content_weight * mse_loss(features_y.relu3_3, features_x.relu3_3)\n",
    "\n",
    "        # Compute gram matrix \n",
    "        style_loss = 0.\n",
    "        for ft_y, gm_s in zip(features_y, gram_style):\n",
    "            gm_y = utils.gram_matrix(ft_y)\n",
    "            style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
    "        style_loss *= style_weight\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if True: #(batch_id + 1) % args.log_interval == 0:\n",
    "            mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                time.ctime(), e + 1, count, len(train_dataset),\n",
    "                              agg_content_loss / (batch_id + 1),\n",
    "                              agg_style_loss / (batch_id + 1),\n",
    "                              (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
    "            )\n",
    "            print(mesg)\n",
    "\n",
    "        if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
    "            transformer.eval().cpu()\n",
    "            ckpt_model_filename = \"ckpt_epoch_\" + str(e) + \"_batch_id_\" + str(batch_id + 1) + \".pth\"\n",
    "            ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n",
    "            torch.save(transformer.state_dict(), ckpt_model_path)\n",
    "            transformer.to(device).train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "transformer.eval().cpu()\n",
    "save_model_filename = \"epoch_\" + str(epochs) + \"_\" + str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "    content_weight) + \"_\" + str(style_weight) + \".model\"\n",
    "save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
    "torch.save(transformer.state_dict(), save_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Image to Stylized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/udnie.pth'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Stylization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = 'mom_berg.png'\n",
    "\n",
    "content_image = utils.load_image(content_image_path, scale=1.0)\n",
    "content_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "content_image = content_transform(content_image)\n",
    "content_image = content_image[:3, :,:]\n",
    "content_image = content_image.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1498, 1374])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_path = 'tst01.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    style_model = TransformerNet()\n",
    "    state_dict = torch.load(model_path)\n",
    "    # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "    for k in list(state_dict.keys()):\n",
    "        if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "            del state_dict[k]\n",
    "    style_model.load_state_dict(state_dict)\n",
    "    style_model.to(device)\n",
    "    output = style_model(content_image).cpu()\n",
    "    utils.save_image(output_image_path, output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     style_model = TransformerNet()\n",
    "#     state_dict = torch.load(model_path)\n",
    "#     # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "\n",
    "#     for k in list(state_dict.keys()):\n",
    "#         if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "#             del state_dict[k]\n",
    "#     style_model.load_state_dict(state_dict)\n",
    "#     style_model.to(device)\n",
    "#     if args.export_onnx:\n",
    "#         assert args.export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n",
    "#         output = torch.onnx._export(style_model, content_image, args.export_onnx).cpu()\n",
    "#     else:\n",
    "#         output = style_model(content_image).cpu()\n",
    "# utils.save_image(args.output_image, output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read ONNX model and run it using Caffe2\n",
    "\"\"\"\n",
    "\n",
    "assert not args.export_onnx\n",
    "\n",
    "import onnx\n",
    "import onnx_caffe2.backend\n",
    "\n",
    "model = onnx.load(args.model)\n",
    "\n",
    "prepared_backend = onnx_caffe2.backend.prepare(model, device='CUDA' if args.cuda else 'CPU')\n",
    "inp = {model.graph.input[0].name: content_image.numpy()}\n",
    "c2_out = prepared_backend.run(inp)[0]\n",
    "\n",
    "return torch.from_numpy(c2_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
